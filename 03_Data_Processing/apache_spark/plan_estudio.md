# ğŸ§­ Plan de Estudio Mensual

**Curso:** Data Engineering
**MÃ³dulo:** 03_Data_Processing
**Tema:** Apache Spark

---

## ğŸ¯ Objetivo del Mes

Comprender los fundamentos, arquitectura y ecosistema de Apache Spark para construir y optimizar procesos de **procesamiento distribuido** (batch y streaming) en grandes volÃºmenes de datos, integrando conceptos de **RDDs**, **DataFrames**, y **Spark SQL** con un enfoque orientado al rendimiento y buenas prÃ¡cticas.

---

## ğŸ§© Resultado Final

Al finalizar el mes, serÃ¡s capaz de **diseÃ±ar y ejecutar un pipeline de procesamiento distribuido** con Apache Spark, realizando transformaciones, agregaciones y anÃ¡lisis sobre grandes datasets, tanto en modo batch como en streaming, con despliegue local o en la nube (AWS EMR, Databricks o Glue).

---

## ğŸ“… Semana 1 â€“ Fundamentos y Arquitectura de Spark

**Objetivo:** Comprender la arquitectura distribuida de Spark y su ecosistema base para procesamiento paralelo.

### Temas Claves

* QuÃ© es Apache Spark y su historia (MapReduce â†’ Spark)
* Arquitectura: Driver, Executor, Cluster Manager
* Conceptos base: RDD, DAG, Transformations vs Actions
* SparkContext, Jobs, Stages, Tasks
* InstalaciÃ³n local o entorno en Databricks Community Edition

### Actividades PrÃ¡cticas

* Instalar PySpark localmente o usar Databricks CE
* Leer un dataset CSV y aplicar transformaciones (`map`, `filter`, `reduceByKey`)
* Visualizar el plan de ejecuciÃ³n (DAG) desde la UI de Spark
* Calcular mÃ©tricas simples (promedios, conteos) con RDDs

---

## ğŸ“… Semana 2 â€“ DataFrames y Spark SQL

**Objetivo:** Usar el modelo estructurado de Spark SQL y DataFrames para anÃ¡lisis declarativos y eficientes.

### Temas Claves

* Limitaciones de RDD â†’ evoluciÃ³n hacia DataFrame y Dataset
* CreaciÃ³n de DataFrames desde CSV, Parquet, JSON
* Operaciones comunes: `select`, `filter`, `groupBy`, `agg`
* *Lazy Evaluation* y optimizador Catalyst
* Consultas SQL sobre DataFrames
* Formatos columnar (Parquet/ORC) y particionamiento

### Actividades PrÃ¡cticas

* Cargar datasets (ventas, logs o sensores) en Parquet y ejecutar queries SQL
* Comparar tiempos entre RDD y DataFrame
* Usar `explain()` para ver el plan lÃ³gico y fÃ­sico
* Crear una tabla temporal y consultar con Spark SQL

---

## ğŸ“… Semana 3 â€“ Spark Streaming y Procesamiento en Tiempo Real

**Objetivo:** Aprender a procesar flujos de datos en tiempo real con Spark Structured Streaming.

### Temas Claves

* Conceptos: micro-batch vs stream continuo
* Fuentes: Kafka, Socket, Files streaming
* *Watermarking*, *windowing* y *checkpointing*
* Operaciones stateful y agregaciones temporales
* IntegraciÃ³n con Kafka â†’ Spark Streaming â†’ sink Parquet/console

### Actividades PrÃ¡cticas

* Crear un streaming job que lea de Kafka o socket (local)
* Aplicar transformaciones (`groupBy`, `count`, `window`)
* Escribir resultados en Parquet o consola
* Visualizar procesamiento en Spark UI en tiempo real

---

## ğŸ“… Semana 4 â€“ OptimizaciÃ³n, Tuning y Proyecto Final

**Objetivo:** Consolidar el conocimiento optimizando jobs y diseÃ±ando un pipeline completo de procesamiento.

### Temas Claves

* *Shuffle*, particiones y *wide transformations*
* ConfiguraciÃ³n de memoria y ejecutores
* *Broadcast joins*, *caching* y *checkpointing*
* *Adaptive Query Execution (AQE)*
* Arquitectura de referencia: Spark en AWS EMR o Databricks

### Actividades PrÃ¡cticas

* Medir rendimiento con y sin cache en DataFrames
* Aplicar *repartition* y *coalesce* para optimizar procesos
* Configurar variables (`spark.sql.shuffle.partitions`, `executor.memory`)
* Ejecutar una pipeline de inicio a fin (ingestiÃ³n â†’ procesamiento â†’ output)

---

## ğŸ’¡ Proyecto Opcional del Mes

**TÃ­tulo:** â€œPipeline de Procesamiento AnalÃ­tico de Ventas con Apache Sparkâ€

**DescripciÃ³n:**
DiseÃ±a un pipeline de procesamiento batch + streaming usando Spark para simular una empresa de retail:

1. **Ingesta:** Leer archivos CSV/JSON desde S3 o local.
2. **Procesamiento Batch:** Limpieza y agregaciÃ³n de ventas por dÃ­a, paÃ­s y producto.
3. **Procesamiento Streaming:** Simular ventas en tiempo real (vÃ­a Kafka o socket).
4. **Output:** Escribir resultados en Parquet y consultar con Spark SQL o Athena.

**Extras (para nivel avanzado):**

* AÃ±adir alertas de fraude en tiempo real usando *window aggregations*.
* Integrar monitoreo con Prometheus + Grafana para Spark metrics.

---

## ğŸ“š Recursos Recomendados

### ğŸ“˜ Libros y DocumentaciÃ³n

* *Learning Spark (2nd Edition)* â€“ Jules Damji (Oâ€™Reilly)
* *High Performance Spark* â€“ Holden Karau & Rachel Warren
* [DocumentaciÃ³n oficial de Apache Spark](https://spark.apache.org/docs/latest/)

### ğŸ¥ Cursos y Tutoriales

* Databricks Academy â€“ *Introduction to Apache Spark*
* Udemy â€“ *Apache Spark for Data Engineers (PySpark)*
* YouTube â€“ Krish Naik / Data Engineer One / Conduktor Academy

### ğŸ§° Herramientas Recomendadas

* **Databricks Community Edition** (ideal para entornos gratuitos)
* **AWS Glue / EMR** para ejecutar Spark en la nube
* **Jupyter + PySpark** para entornos locales de aprendizaje
* **Spark UI + Prometheus/Grafana** para monitoreo y tuning

---

Â¿Quieres que te genere tambiÃ©n una **versiÃ³n diaria (lunes a viernes)** para este mismo plan â€” ideal si estudiarÃ¡s 1 hora por dÃ­a con teorÃ­a + prÃ¡ctica guiada?
