# üß≠ Plan de Estudio Mensual

**Curso:** Data Engineering
**M√≥dulo:** 02_Data_Ingestion
**Tema:** Apache Kafka

---

## üéØ Objetivo del Mes

Comprender la arquitectura, fundamentos y operaciones clave de **Apache Kafka** para construir **pipelines de ingesti√≥n y streaming de datos** robustos, escalables y orientados a eventos, aplicables en ecosistemas de Data Engineering y microservicios.

---

## üß© Resultado Final

Al finalizar el mes, podr√°s **dise√±ar e implementar un flujo de ingesti√≥n en tiempo real** usando Apache Kafka, con productores, consumidores, topics y configuraciones b√°sicas de persistencia y particionamiento, integrando conceptos como **producers/consumers**, **brokers**, **offsets**, **partitions**, y **Kafka Connect**.

---

## üìÖ Semana 1 ‚Äì Fundamentos y Arquitectura de Kafka

**Objetivo:** Entender qu√© es Apache Kafka, su arquitectura distribuida y el rol que cumple en la ingesti√≥n de datos modernos.

### Temas Clave

* Qu√© es Apache Kafka y su caso de uso principal
* Componentes esenciales: Broker, Topic, Partition, Offset
* Conceptos de *Publish‚ÄìSubscribe* y *Stream Processing*
* Comparaci√≥n con RabbitMQ, Kinesis y Pulsar
* Instalaci√≥n local o uso de Docker

### Actividades Pr√°cticas

* Instalar Kafka y ZooKeeper localmente o con Docker Compose
* Crear un *topic* y verificar su existencia con `kafka-topics.sh`
* Producir y consumir mensajes desde consola (`kafka-console-producer.sh` y `kafka-console-consumer.sh`)
* Visualizar flujo de datos b√°sico entre productor y consumidor

---

## üìÖ Semana 2 ‚Äì Productores, Consumidores y Configuraci√≥n

**Objetivo:** Aprender c√≥mo interact√∫an los productores y consumidores, c√≥mo se gestiona la durabilidad y el rendimiento del sistema.

### Temas Clave

* Productor: acknowledgments, retries, key-partitioning
* Consumidor: grupos, offsets, commits manuales/autom√°ticos
* Concepto de *at-most-once*, *at-least-once*, *exactly-once*
* Particionamiento y replicaci√≥n: claves del escalamiento horizontal

### Actividades Pr√°cticas

* Crear un **productor** y **consumidor** simples en Python o JavaScript
* Configurar *acks* y *retries* en el productor
* Probar un *consumer group* con dos instancias y observar la repartici√≥n
* Medir la entrega garantizada de mensajes (*at-least-once*)

---

## üìÖ Semana 3 ‚Äì Ecosistema Kafka y Procesamiento de Datos

**Objetivo:** Conocer las extensiones y herramientas del ecosistema Kafka para ingesti√≥n y procesamiento avanzado.

### Temas Clave

* Kafka Connect: conectores *source* y *sink*
* Kafka Streams y su modelo de procesamiento
* Schema Registry (Avro/JSON) y control de compatibilidad
* Integraci√≥n con Spark, Flink o KSQL (Kafka SQL Engine)
* Casos reales: pipelines en tiempo real, ETL streaming

### Actividades Pr√°cticas

* Configurar **Kafka Connect** con un conector de archivo CSV o base de datos (ej. PostgreSQL ‚Üí Kafka)
* Crear un flujo con **Kafka Streams** o **KSQL** para filtrar y transformar datos
* Visualizar el flujo con una herramienta como **Kafdrop** o **Conduktor**

---

## üìÖ Semana 4 ‚Äì Escalabilidad, Tuning y Proyecto Final

**Objetivo:** Consolidar los conocimientos aplicando mejores pr√°cticas de rendimiento y dise√±o de arquitecturas robustas.

### Temas Clave

* Particiones, r√©plicas y tolerancia a fallos
* Monitoreo y m√©tricas con Prometheus + Grafana
* Retenci√≥n de datos y limpieza de logs
* Seguridad b√°sica: autenticaci√≥n, ACLs y cifrado SSL
* Buenas pr√°cticas de dise√±o en entornos productivos

### Actividades Pr√°cticas

* Configurar m√©tricas de Kafka y visualizar en Grafana
* Probar diferentes pol√≠ticas de retenci√≥n (por tama√±o y tiempo)
* Simular fallas de un broker y verificar tolerancia
* Optimizar throughput modificando *batch size*, *linger.ms* y *compression.type*

---

## üí° Proyecto Opcional del Mes

**T√≠tulo:** ‚ÄúPipeline de Ingesti√≥n de Transacciones en Tiempo Real con Apache Kafka‚Äù

**Descripci√≥n:**
Dise√±a un pipeline completo que simule el flujo de transacciones bancarias o eventos de IoT en tiempo real:

1. **Productor:** genera eventos JSON (transacciones, sensores, logs).
2. **Kafka Cluster:** recibe, particiona y persiste mensajes.
3. **Kafka Streams / KSQL:** filtra y agrega estad√≠sticas (ej. fraude, temperatura, etc.).
4. **Consumidor final:** muestra resultados en consola, base de datos o dashboard.

**Extras (si tienes m√°s tiempo):**

* A√±adir Kafka Connect para enviar los datos a PostgreSQL o S3.
* A√±adir monitoreo con Prometheus y Grafana.

---

## üìö Recursos Recomendados

### Libros y Documentaci√≥n

* *Kafka: The Definitive Guide* ‚Äî Neha Narkhede, Gwen Shapira, Todd Palino (O‚ÄôReilly)
* *Designing Event-Driven Systems* ‚Äî Ben Stopford
* [Documentaci√≥n oficial de Apache Kafka](https://kafka.apache.org/documentation/)

### Cursos y Videos

* üé• Confluent Developer: *Apache Kafka Fundamentals*
* üé• Data Engineering on YouTube (Simplilearn, Conduktor Academy)
* üß† Udemy: *Apache Kafka Series ‚Äì Learn Apache Kafka for Beginners*

### Herramientas √ötiles

* **Docker Compose:** para levantar un cl√∫ster local r√°pido
* **Kafdrop / Conduktor:** visualizaci√≥n de topics y mensajes
* **Prometheus + Grafana:** monitoreo de brokers y lag de consumidores
